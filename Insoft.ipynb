{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install openai==0.28"
      ],
      "metadata": {
        "id": "nVYL3b4MaVxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import gradio as gr\n",
        "openai.api_key = 'API'  # Replace with your OpenAI API key"
      ],
      "metadata": {
        "id": "8IeV8FMtu-bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predefined dataset\n",
        "predefined_responses = {\n",
        "    \"What does the eligibility verification agent (EVA) do?\": \"EVA automates the process of verifying a patientâ€™s eligibility and benefits information in real-time, eliminating manual data entry errors and reducing claim rejections.\",\n",
        "    \"What does the claims processing agent (CAM) do?\": \"CAM streamlines the submission and management of claims, improving accuracy, reducing manual intervention, and accelerating reimbursements.\",\n",
        "    \"How does the payment posting agent (PHIL) work?\": \"PHIL automates the posting of payments to patient accounts, ensuring fast, accurate reconciliation of payments and reducing administrative burden.\",\n",
        "    \"Tell me about Insoft AI's Agents.\": \"Insoft AI provides a suite of AI-powered automation agents designed to streamline healthcare processes. These include Eligibility Verification (EVA), Claims Processing (CAM), and Payment Posting (PHIL), among others.\",\n",
        "    \"What are the benefits of using Insoft AI's agents?\": \"Using Insoft AI's Agents can significantly reduce administrative costs, improve operational efficiency, and reduce errors in critical processes like claims management and payment posting.\"\n",
        "}\n",
        "\n",
        "def get_response(question):\n",
        "    try:\n",
        "        # Check for predefined response\n",
        "        if question in predefined_responses:\n",
        "            return predefined_responses[question]\n",
        "        else:\n",
        "            # Fallback to LLM response\n",
        "            return get_llm_response(question)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Function to get LLM response using OpenAI API\n",
        "def get_llm_response(question):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error retrieving LLM response: {str(e)}\"\n",
        "\n",
        "def chat_interface(question):\n",
        "    # Check if the input is valid\n",
        "    if not isinstance(question, str) or not question.strip():\n",
        "        return \"Invalid input. Please enter a valid question.\"\n",
        "    return get_response(question)\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Insoft AI Support Agent\",\n",
        "    description=\"Ask questions about Insoft AI's agents and get instant responses!\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Launch the interface\n",
        "    interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "icgfN25Gmk6q",
        "outputId": "0db6a51c-4496-4549-e45c-2a487c7e23a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3bc1bbb4344d94f8e5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3bc1bbb4344d94f8e5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P2wi9Z80bDb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7mzCYw8vEzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}